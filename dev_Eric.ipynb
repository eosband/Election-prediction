{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVj1JpqUWNp1"
   },
   "source": [
    "<h2>CS 4780/5780 Final Project: </h2>\n",
    "<h3>Election Result Prediction for US Counties</h3>\n",
    "\n",
    "Names and NetIDs for your group members: Eric Osband (eo255), Anthony Cuturuffo (acc284), Eddie Freedman (ebf45???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Y-2-S5WNp1"
   },
   "source": [
    "<h3>Introduction:</h3>\n",
    "\n",
    "<p> The final project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The programming project provide templates for how to do this, and the most recent video lectures summarize some of the tricks you will need (e.g. feature normalization, feature construction). So, this final project brings realism to how you will use machine learning in the real world.  </p>\n",
    "\n",
    "The task you will work on is forecasting election results. Economic and sociological factors have been widely used when making predictions on the voting results of US elections. Economic and sociological factors vary a lot among counties in the United States. In addition, as you may observe from the election map of recent elections, neighbor counties show similar patterns in terms of the voting results. In this project you will bring the power of machine learning to make predictions for the county-level election results using Economic and sociological factors and the geographic structure of US counties. </p>\n",
    "<p>\n",
    "\n",
    "<h3>Your Task:</h3>\n",
    "Plase read the project description PDF file carefully and make sure you write your code and answers to all the questions in this Jupyter Notebook. Your answers to the questions are a large portion of your grade for this final project. Please import the packages in this notebook and cite any references you used as mentioned in the project description. You need to print this entire Jupyter Notebook as a PDF file and submit to Gradescope and also submit the ipynb runnable version to Canvas for us to run.\n",
    "\n",
    "<h3>Due Date:</h3>\n",
    "The final project dataset and template jupyter notebook will be due on <strong>December 15th</strong> . Note that <strong>no late submissions will be accepted</strong>  and you cannot use any of your unused slip days before.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jjwk83POWNp1"
   },
   "source": [
    "![image.png; width=\"100\";](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdQ0GV-eWNp1"
   },
   "source": [
    "<h2>Part 1: Basics</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFL54oJVWNp1"
   },
   "source": [
    "<h3>1.1 Import:</h3><p>\n",
    "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "    \n",
    "https://pytorch.org/tutorials/\n",
    "    \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mQDt73qwWNp1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# TODO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJGUskA_WNp1"
   },
   "source": [
    "<h3>1.2 Weighted Accuracy:</h3><p>\n",
    "Since our dataset labels are heavily biased, you need to use the following function to compute weighted accuracy throughout your training and validation process and we use this for testing on Kaggle.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vz7GtRvuWNp1"
   },
   "outputs": [],
   "source": [
    "def weighted_accuracy(pred, true):\n",
    "    assert(len(pred) == len(true))\n",
    "    num_labels = len(true)\n",
    "    num_pos = sum(true)\n",
    "    num_neg = num_labels - num_pos\n",
    "    frac_pos = num_pos/num_labels\n",
    "    weight_pos = 1/frac_pos\n",
    "    weight_neg = 1/(1-frac_pos)\n",
    "    num_pos_correct = 0\n",
    "    num_neg_correct = 0\n",
    "    for pred_i, true_i in zip(pred, true):\n",
    "        num_pos_correct += (pred_i == true_i and true_i == 1)\n",
    "        num_neg_correct += (pred_i == true_i and true_i == 0)\n",
    "    weighted_accuracy = ((weight_pos * num_pos_correct) \n",
    "                         + (weight_neg * num_neg_correct))/((weight_pos * num_pos) + (weight_neg * num_neg))\n",
    "    return weighted_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd8F4FVTWNp2"
   },
   "source": [
    "<h2>Part 2: Baseline Solution</h2><p>\n",
    "Note that your code should be commented well and in part 2.4 you can refer to your comments. (e.g. # Here is SVM, \n",
    "# Here is validation for SVM, etc). Also, we recommend that you do not to use 2012 dataset and the graph dataset to reach the baseline accuracy for 68% in this part, a basic solution with only 2016 dataset and reasonable model selection will be enough, it will be great if you explore thee graph and possibly 2012 dataset in Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4STDPiVFWNp2"
   },
   "source": [
    "<h3>2.1 Preprocessing and Feature Extraction:</h3><p>\n",
    "Given the training dataset and graph information, you need to correctly preprocess the dataset (e.g. feature normalization). For baseline solution in this part, you might not need to introduce extra features to reach the baseline test accuracy.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "xW-Vh-OXNbLh",
    "outputId": "c0adfbb9-9605-403f-feac-d2a9ce031d2c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>County</th>\n",
       "      <th>DEM</th>\n",
       "      <th>GOP</th>\n",
       "      <th>MedianIncome</th>\n",
       "      <th>MigraRate</th>\n",
       "      <th>BirthRate</th>\n",
       "      <th>DeathRate</th>\n",
       "      <th>BachelorRate</th>\n",
       "      <th>UnemploymentRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18019</td>\n",
       "      <td>Clark County, IN</td>\n",
       "      <td>18791</td>\n",
       "      <td>30012</td>\n",
       "      <td>51,837</td>\n",
       "      <td>4.9</td>\n",
       "      <td>12.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6035</td>\n",
       "      <td>Lassen County, CA</td>\n",
       "      <td>2026</td>\n",
       "      <td>6533</td>\n",
       "      <td>49,793</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>9.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40081</td>\n",
       "      <td>Lincoln County, OK</td>\n",
       "      <td>2423</td>\n",
       "      <td>10838</td>\n",
       "      <td>44,914</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31153</td>\n",
       "      <td>Sarpy County, NE</td>\n",
       "      <td>27704</td>\n",
       "      <td>44649</td>\n",
       "      <td>74,374</td>\n",
       "      <td>9.2</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.1</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28055</td>\n",
       "      <td>Issaquena County, MS</td>\n",
       "      <td>395</td>\n",
       "      <td>298</td>\n",
       "      <td>26,957</td>\n",
       "      <td>-12.8</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FIPS                County    DEM    GOP MedianIncome  MigraRate  \\\n",
       "0  18019      Clark County, IN  18791  30012       51,837        4.9   \n",
       "1   6035     Lassen County, CA   2026   6533       49,793      -18.4   \n",
       "2  40081    Lincoln County, OK   2423  10838       44,914       -1.3   \n",
       "3  31153      Sarpy County, NE  27704  44649       74,374        9.2   \n",
       "4  28055  Issaquena County, MS    395    298       26,957      -12.8   \n",
       "\n",
       "   BirthRate  DeathRate  BachelorRate  UnemploymentRate  \n",
       "0       12.8       11.0          20.9               4.2  \n",
       "1        9.2        6.3          12.0               6.9  \n",
       "2       11.4       11.7          15.1               5.3  \n",
       "3       14.2        5.0          40.1               2.9  \n",
       "4        9.8        5.3           6.7              14.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may change this but we suggest loading data with the following code and you may need to change\n",
    "# datatypes and do necessary data transformation after loading the raw data to the dataframe.\n",
    "dataset_path = \"./train_2016.csv\"\n",
    "# df = pd.read_csv(dataset_path, sep=',',header=None, encoding='unicode_escape')\n",
    "\n",
    "# Chose to include header to remember column identifiers\n",
    "df = pd.read_csv(dataset_path, sep=',', encoding='unicode_escape')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "O7bsqEl8WNp2",
    "outputId": "8fa672e5-45b1-4982-f259-1ffe5710947d"
   },
   "outputs": [],
   "source": [
    "actual_pred = lambda x : np.sign(x * 2)\n",
    "def preprocess(raw_df):\n",
    "    df = raw_df.copy()\n",
    "    # Create feature representing state number\n",
    "    # Gets state initials from a county string\n",
    "    get_state_from_county = lambda county : county[county.index(\",\") + 2:]\n",
    "  \n",
    "    df[\"state_name\"] = df[\"County\"].apply(get_state_from_county)\n",
    "    states = df[\"state_name\"].unique().tolist()\n",
    "  \n",
    "    \n",
    "    \n",
    "    #one-hot encode state data\n",
    "    onehot = pd.get_dummies(df[\"state_name\"], prefix = None)\n",
    "    df[onehot.columns] = onehot\n",
    "    \n",
    "    #create target label\n",
    "    target = \"target\" # Percentage DEM vote, range between 0 and 1. Apply actual_pred(df[\"target\"]) to get actual 0-1 prediction\n",
    "    df[target] = (df[\"DEM\"] / (df[\"DEM\"] + df[\"GOP\"])).astype(float)\n",
    "    \n",
    "    # Get rid of all commas in MedianIncome column\n",
    "    df['MedianIncome']=df['MedianIncome'].str.replace(',','').astype(int)\n",
    "\n",
    "    # Get rid of county, state_name, DEM and GOP columns\n",
    "    df = df.drop(columns = [\"state_name\", \"County\", \"DEM\", \"GOP\", \"FIPS\"])\n",
    "    \n",
    "    y = df[\"target\"].to_numpy()\n",
    "    df = df.drop(columns = [\"target\"])\n",
    "    x = df.to_numpy()\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1555, 56)\n"
     ]
    }
   ],
   "source": [
    "x, y = preprocess(df)\n",
    "sample_size, input_size = x.shape\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor, y_tensor = torch.from_numpy(x).float(), torch.from_numpy(y).long()\n",
    "x_tensor = torch.nn.functional.normalize(x_tensor, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(x_tensor, y_tensor)\n",
    "train, test = torch.utils.data.random_split(dataset, [1000, 555])\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True,batch_size=1)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running epochs...:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running batches...:   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Running epochs...:   0%|          | 0/100 [00:00<?, ?it/s]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x56 and 42x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-a219d8e7aeae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;31m# epoch. tqdm gives a nice progress bar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Running epochs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# Suppose we wanted to find the validation accuracy after epoch! All we'd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-a219d8e7aeae>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m    193\u001b[0m                                 \u001b[0;31m#   the current batch. Therefore, we zero out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                                 \u001b[0;31m#   the gradient of the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# Compute predictions for a batch 'x'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute the loss function's value on the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                                 \u001b[0;31m#   outputs of the model on 'x' and the true\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-a219d8e7aeae>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# it as fc(r1(f1(x)))))!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x56 and 42x2)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Three step process for machine learning, ignoring a bunch of important things\n",
    "we talked about in class:\n",
    "\n",
    "1. Get data\n",
    "    1.1 Get a Dataset---this is provided by PyTorch\n",
    "    1.2 Get a DataLoader---this is provided by PyTorch\n",
    "2. Get a classifier, and other paraphenalia needed for training\n",
    "    2.1 Create a classifier architecture, and instantiate it\n",
    "    2.2 Get a criterion (loss function)\n",
    "    2.3 Get an optimizer (thing that does parameter updates to the classifier)\n",
    "3. Classify the data in some reasonably intelligent fashion. This will involve:\n",
    "    3.1 A one_epoch() function that trains the classifier for one epoch---a\n",
    "        complete iteration over the dataset from (1.1)\n",
    "    3.2 A loop that calls one_epoch() the number of times we want to run an\n",
    "        epoch.\n",
    "4. ...\n",
    "5. $$$\n",
    "\"\"\"\n",
    "# Have you ever *not* needed NumPy?\n",
    "import numpy as np\n",
    "\n",
    "# We need to import a bunch of things from PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# tqdm gives us nice progress bars, which provide a huge quality-of-life boost,\n",
    "# so it's worth mentioning here.\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 'something_from_torch = something_from_torch.to(device)' puts the thing onto\n",
    "# the device. If you have CUDA available, this will massively speed\n",
    "# computations. Note that all inputs to a computation must be on the same\n",
    "# device, and that things are on the CPU by default.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# train_dataset = Subset(dataset, range(0, 1000))\n",
    "# val_dataset = Subset(dataset, range(1000, 1100))\n",
    "# val_loader = DataLoader(val_dataset, batch_size=2, num_workers=4)\n",
    "# input_dim, output_dim, num_epochs = input_size, 2, 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t: torch.flatten(t)),\n",
    "])\n",
    "dataset = torchvision.datasets.MNIST(\"./MNIST/\",\n",
    "    train=True, transform=transform, download=True)\n",
    "train_dataset = Subset(dataset, range(0, 10000))\n",
    "val_dataset = Subset(dataset, range(10000, 11000))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=4)\n",
    "input_dim, output_dim, num_epochs = 784, 10, 100\n",
    "\n",
    "\n",
    "# Setting a random seed means we can reproduce our results easily. In my\n",
    "# understanding, this is generally good practice.\n",
    "np.random.seed(1701)\n",
    "torch.manual_seed(1701)\n",
    "\n",
    "# Here I define a bunch of constants that'll be needed later on. I've chosen the\n",
    "# first two arbitrarily; the last three are set to values that are probably\n",
    "# decent across a wide range of tasks.\n",
    "input_dim = 42          # This is the dimensionality of the inputs X\n",
    "output_dim = 314159     # This is the number of classes in Y\n",
    "num_workers = 4         # We want to parallelize loading data during training.\n",
    "                        #   Otherwise, it's possible for moving data to model to\n",
    "                        #   become a bottleneck in performance!\n",
    "batch_size = 16         # This is the number of examples (x,y) that are run\n",
    "                        #   through the neural net simultaneously. Think of it as\n",
    "                        #   a mid-ground between SGD and GD.\n",
    "num_epochs = 100        # The number of complete iterations over the dataset\n",
    "                        #   made during training\n",
    "\n",
    "save_file = \"best_model.pt\" # It's helpful to save the best-performing model\n",
    "                            #   throughout training...\n",
    "\n",
    "# 1.1---get the Dataset. A Dataset should extend PyTorch's base class, and needs\n",
    "# to do two things:\n",
    "# a) implement the __getitem__() method. This takes in an index to the data and\n",
    "#       returns the x- and y-values of the ith datapoint, often as tensors.\n",
    "# b) implement the __len__() method. This is just the amount of data in the\n",
    "#       dataset.\n",
    "class ToyDataset(Dataset):\n",
    "    \"\"\"An example dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "# train_dataset = ToyDataset()\n",
    "\n",
    "# 1.2---get a DataLoader. This wraps a Dataset and allows you to iterate through\n",
    "# it. It's also where you can implement *a lot* of performance optimizations.\n",
    "# 'shuffle=True' is set by default, but it's worth emphasizing here.\n",
    "#\n",
    "# As long as values for 'batch_size' are reasonable (4 <= x << amount of data),\n",
    "# changing it will mostly change the speed and not performance of the model.\n",
    "# 'num_workers' should be just under your number of CPUs.\n",
    "# train_loader = DataLoader(train_dataset,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size,\n",
    "#     num_workers=num_workers)\n",
    "    \n",
    "# 2.1---here we define the actual neural net! It should be a class that inherits\n",
    "# from torch.nn.Module, and has a forward() method that implements the function\n",
    "#\n",
    "#       f : X -> Y\n",
    "#\n",
    "# Because the neural net is *literally* a function, I'll call its outputs on an\n",
    "# input 'x' 'fx'.\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    # As you know, a neural net is composed of layers. The first layer takes in\n",
    "    # an example x, does a computation on it, and passes the result fx to the\n",
    "    # second layer, and this process continues.\n",
    "    #\n",
    "    # Note that the architecture below illustrates how to build a neural net,\n",
    "    # but shouldn't work too well otherwise.\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.f1 = nn.Linear(input_dim , 2)  # The first layer should map from\n",
    "                                            #   the dimensionality of the input\n",
    "                                            #   data to another dimensionality.\n",
    "        self.r1 = nn.ReLU()                 # Activation functions operate\n",
    "                                            #   element-wise, so they don't need\n",
    "                                            #   a dimensionality specified, and\n",
    "                                            #   they don't change the\n",
    "                                            #   dimensionality of their inputs.\n",
    "                                            #   Generally every linear layer is\n",
    "                                            #   followed by an activation\n",
    "                                            #   function.\n",
    "        self.fc = nn.Linear(2, output_dim)  # The last layer should map from the\n",
    "                                            #   last hidden dimensionality to\n",
    "                                            #   the desired number of classes.\n",
    "                                            #   At least in computer vision,\n",
    "                                            #   it's generally called 'fc'.\n",
    "                                            \n",
    "    # Don't worry about adding an activation function at the end. It's done\n",
    "    # automatically in the loss function! Of course, this means the outputs of\n",
    "    # the model need to be softmaxed before they can be used as probabilities.\n",
    "    \n",
    "    # This is the function that implements f : X -> Y. Note that we can expand\n",
    "    # it as fc(r1(f1(x)))))!\n",
    "    def forward(self, x):\n",
    "        fx = self.f1(x)\n",
    "        fx = self.r1(fx)\n",
    "        return self.fc(fx)\n",
    "        \n",
    "# Instantiate the model! We need to move it to whatever device it'll be running\n",
    "# on before we instantiate the optimizer in 2.3.\n",
    "model = NeuralNet(input_dim, output_dim).to(device)\n",
    "\n",
    "# 2.2---The criterion is the loss function. Normally, you use CrossEntropy for a\n",
    "# classification task. (Binary classification is a special case where you can\n",
    "# get away with doing something else like 0-1 loss.)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2.3---The optimizer takes care of the parameter updates via its step() method.\n",
    "# To make it work, we need to pass in the 'parameters' of the model. Generally,\n",
    "# you'll choose between Adam and SGD.\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 3.1---I find it's useful to define a function implementing a single epoch, and\n",
    "# then call it repeatedly from within a loop to train the neural net.\n",
    "def one_epoch(model, train_loader, optimizer, criterion):\n",
    "    \"\"\"Returns [model] after being trained for one epoch on [data_loader] using\n",
    "    [optimizer] and [criterion].\n",
    "    \"\"\"\n",
    "    # [data_loader] returns batches of paired x- and y-values when we iterate\n",
    "    # over it. Because of PyTorch magic, we don't have to worry about the batch\n",
    "    # size!\n",
    "    #\n",
    "    # What is tqdm doing here? When we wrap an iterable in tqdm() during\n",
    "    # iteration, we get a super nice progress bar!\n",
    "    for x,y in tqdm(train_loader, desc=\"Running batches...\", leave=False):\n",
    "        # x is a tensor of dimension (batch_size x n_features), y is a tensor of\n",
    "        # dimension (batch_size,). If they're not on the same device as [model],\n",
    "        # they need to be moved there. (Note that it might be smarter to\n",
    "        # accomplish this through the DataLoader.)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()   # We only want to accumulate the gradient for\n",
    "                                #   the current batch. Therefore, we zero out\n",
    "                                #   the gradient of the model.\n",
    "        fx = model(x)           # Compute predictions for a batch 'x'\n",
    "        loss = criterion(fx, y) # Compute the loss function's value on the\n",
    "                                #   outputs of the model on 'x' and the true\n",
    "                                #   labels 'y'\n",
    "        loss.backward()         # Compute the gradients of the model's weights\n",
    "                                #   with respect to the loss.\n",
    "        optimizer.step()        # Update parameters by taking an intelligent\n",
    "                                #   optimizer-determined step against the\n",
    "                                #   gradient.\n",
    "        \n",
    "    # When the epoch is over, return the model.\n",
    "    return model\n",
    "\n",
    "# We can keep track of when after an epoch finishes our model has a better\n",
    "# validation accuracy than anything else using this. We'll also need a function\n",
    "# to compute validation accuracy!\n",
    "best_val_acc = float(\"-inf\")\n",
    "\n",
    "def validate(model, val_loader):\n",
    "    \"\"\"Returns the accuracy of [model] on [val_loader].\"\"\"\n",
    "    \n",
    "    def batch_acc(x, y):\n",
    "        \"\"\"Returns the number of correct predictions of [model] on [x] given\n",
    "        [y].\n",
    "        \"\"\"\n",
    "        # Note that PyTorch can do most things that NumPy can. model(x) returns\n",
    "        # a batch of predictions as a (batch_size x n_classes) tensor; y is a\n",
    "        # (n_examples,) tensor of in which the i^th index contains the class of\n",
    "        # the i^th example, eg. 5.\n",
    "        #\n",
    "        # Additionally, [model] is on [device], so you'll need to move [x] to\n",
    "        # the device prior to computation. To move a tensor [t] to the CPU you\n",
    "        # can call 't.cpu()'---this might be useful for comparing with [y].\n",
    "        #\n",
    "        # Can you fit this all on one line?\n",
    "        x.cpu()\n",
    "        return y.eq(model(x)).sum()\n",
    "    \n",
    "    # The length of a Dataset is the number of examples in it; the length of a\n",
    "    # DataLoader is the number of batches. Therefore, to get the accuracy, it's\n",
    "    # critical to divide by the length of the first and not the second!\n",
    "    #\n",
    "    # We can use NumPy here because it's generally (in my experience) faster for\n",
    "    # things on the CPU than PyTorch.\n",
    "    return np.sum([batch_acc(x, y) for x,y in val_loader]) / len(val_dataset)\n",
    "\n",
    "# 3.2---Train the model on the entirety of the data once for every desired\n",
    "# epoch. tqdm gives a nice progress bar.\n",
    "for e in tqdm(range(num_epochs), desc=\"Running epochs...\"):\n",
    "    model = one_epoch(model, train_loader, optimizer, criterion)\n",
    "    \n",
    "    # Suppose we wanted to find the validation accuracy after epoch! All we'd\n",
    "    # need would be a validation DataLoader, and a function to get accuracy\n",
    "    # from! Putting this under 'with torch.no_grad():' turns off computing\n",
    "    # gradients, because we don't need them. Moreover, wrapping it in\n",
    "    # 'model.eval()' and 'model.train()' is important if we do fancy things like\n",
    "    # dropout.\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_acc = validate(model, val_loader)\n",
    "        model.train()\n",
    "    \n",
    "    # When in the middle of a tqdm-ed loop, we need to use 'tqdm.write()'\n",
    "    # instead of 'print()'.\n",
    "    tqdm.write(f\"Epoch {e:3} | validation accuracy: {val_acc}\")\n",
    "    \n",
    "    # If the validation accuracy is the highest it's ever been, why not save the\n",
    "    # the model? This is a natural way to figure out the correct number of\n",
    "    # epochs!\n",
    "    if val_acc > best_val_acc:\n",
    "        tqdm.write(f\"------Best epoch yet with accuracy {val_acc}. Saved!\")\n",
    "        best_val_acc = val_acc\n",
    "        \n",
    "        # Saving a PyTorch model is super easy! 'model.state_dict()' converts\n",
    "        # all of its weights to a dictionary format. The general format here is\n",
    "        # 'torch.save(dictionary, file)'. Note that we can put basically\n",
    "        # anything we want in this dictionary too!\n",
    "        torch.save(model.state_dict(), save_file)\n",
    "        \n",
    "# Now that training is done, let's load the best model so we can do something\n",
    "# with it. First, we need to instantiate a new model of the same type as the\n",
    "# old one. Then we can call 'load_state_dict()' on the loaded state_dict.\n",
    "model = NeuralNet(input_dim, output_dim)\n",
    "best_val_acc_model_weights = torch.load(save_file)\n",
    "model.load_state_dict(best_val_acc_model_weights)\n",
    "\n",
    "################################################################################\n",
    "### WANT TO SEE THE CODE RUN? PASTE THE FOLLOWING CODE DIRECTLY ABOVE 1.2, THEN\n",
    "### IMPLEMENT 'batch_acc()'.\n",
    "### It's way beyond the scope of the course though still far too simple for the\n",
    "### task it takes on; I use it to validate what I've written elsewhere. Also\n",
    "### note that it downloads the MNIST (handwritten images) dataset to your\n",
    "### computer, so beware...though you really can just delete it later.\n",
    "###\n",
    "### Aside: you should be able to get at least 95% accuracy on the MNIST dataset\n",
    "### without trying. This gets up to about 70% accuracy because I deliberately\n",
    "### wrote the worst neural net I could think of that'd illustrate what I needed\n",
    "### to show.\n",
    "################################################################################\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Lambda(lambda t: torch.flatten(t)),\n",
    "# ])\n",
    "# dataset = torchvision.datasets.MNIST(\"./MNIST/\",\n",
    "#     train=True, transform=transform, download=True)\n",
    "# train_dataset = Subset(dataset, range(0, 10000))\n",
    "# val_dataset = Subset(dataset, range(10000, 11000))\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, num_workers=4)\n",
    "# input_dim, output_dim, num_epochs = 784, 10, 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.layer_out = torch.nn.Linear(num_classes, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        #out = self.relu(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        #out = self.relu(out)\n",
    "        out = self.layer_out(out)\n",
    "        #out = torch.nn.functional.log_softmax(out)\n",
    "        #out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [199/1000 (20%)]\tLoss: 16.482794\n",
      "Train Epoch: 0 [399/1000 (40%)]\tLoss: 27.657545\n",
      "Train Epoch: 0 [599/1000 (60%)]\tLoss: 8.843080\n",
      "Train Epoch: 0 [799/1000 (80%)]\tLoss: 0.319080\n",
      "Train Epoch: 0 [999/1000 (100%)]\tLoss: 5.631245\n",
      "Train Epoch: 1 [199/1000 (20%)]\tLoss: 15.564516\n",
      "Train Epoch: 1 [399/1000 (40%)]\tLoss: 28.379908\n",
      "Train Epoch: 1 [599/1000 (60%)]\tLoss: 8.679072\n",
      "Train Epoch: 1 [799/1000 (80%)]\tLoss: 0.284984\n",
      "Train Epoch: 1 [999/1000 (100%)]\tLoss: 5.707983\n",
      "Train Epoch: 2 [199/1000 (20%)]\tLoss: 15.328143\n",
      "Train Epoch: 2 [399/1000 (40%)]\tLoss: 28.585480\n",
      "Train Epoch: 2 [599/1000 (60%)]\tLoss: 8.622234\n",
      "Train Epoch: 2 [799/1000 (80%)]\tLoss: 0.270438\n",
      "Train Epoch: 2 [999/1000 (100%)]\tLoss: 5.742008\n",
      "Train Epoch: 3 [199/1000 (20%)]\tLoss: 15.219230\n",
      "Train Epoch: 3 [399/1000 (40%)]\tLoss: 28.682671\n",
      "Train Epoch: 3 [599/1000 (60%)]\tLoss: 8.593343\n",
      "Train Epoch: 3 [799/1000 (80%)]\tLoss: 0.262439\n",
      "Train Epoch: 3 [999/1000 (100%)]\tLoss: 5.761169\n",
      "Train Epoch: 4 [199/1000 (20%)]\tLoss: 15.156590\n",
      "Train Epoch: 4 [399/1000 (40%)]\tLoss: 28.739197\n",
      "Train Epoch: 4 [599/1000 (60%)]\tLoss: 8.575919\n",
      "Train Epoch: 4 [799/1000 (80%)]\tLoss: 0.257399\n",
      "Train Epoch: 4 [999/1000 (100%)]\tLoss: 5.773431\n"
     ]
    }
   ],
   "source": [
    "num_classes = hidden_size = input_size\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "epochs = 5\n",
    "learning_rate = .001\n",
    "\n",
    "# Loss and optimizer\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean((10*(output - target))**2)\n",
    "    return loss\n",
    "criterion = my_loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "#Training model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "        #print(data)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        net_out = model(data)\n",
    "        #print(net_out,target)\n",
    "        loss = criterion(net_out, target)\n",
    "        \n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 199:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                               100. * batch_idx / len(train_loader), loss.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 7.1631, Accuracy: 478/555 (86%)\n",
      "\n",
      "\n",
      "Weighted accuracy: []\n",
      "tensor([[0.1141]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#TESTING\n",
    "# run a test loop\n",
    "convert_pred = lambda x : (torch.sign(x - 0.5) + 1) / 2\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "preds = np.array([])\n",
    "for data, target in test_loader:\n",
    "    data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "    net_out = model(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target)\n",
    "    pred = convert_pred(net_out.data.max(1)[1])  # get the index of the max log-probability\n",
    "#     print(net_out, pred, target)\n",
    "#     print(target.data[0])\n",
    "#     print(np.sign(0.9))\n",
    "#     print(pred.numpy())\n",
    "#     preds.append([pred.numpy()])\n",
    "    correct += pred.eq(convert_pred(target.data)).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "print('\\nWeighted accuracy:', preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG-1TlocWNp2"
   },
   "source": [
    "<h3>2.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
    "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE0InbZ7WNp2"
   },
   "outputs": [],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 2.4\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbJ4kRDhWNp2"
   },
   "source": [
    "<h3>2.3 Training, Validation and Model Selection:</h3><p>\n",
    "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3mQNv56WNp2"
   },
   "outputs": [],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 2.4\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuOP9JmuWNp2"
   },
   "source": [
    "<h3>2.4 Explanation in Words:</h3><p>\n",
    "    You need to answer the following questions in the markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHiJ7drqWNp2"
   },
   "source": [
    "2.4.1 How did you preprocess the dataset and features?\n",
    "\n",
    "2.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
    "\n",
    "2.4.3 How did you do the model selection?\n",
    "\n",
    "2.4.4 Does the test performance reach a given baseline 68% performanc? (Please include a screenshot of Kaggle Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pwkT1-tWNp2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2JwrNuEWNp2"
   },
   "source": [
    "<h2>Part 3: Creative Solution</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HttJ6XeDWNp2"
   },
   "source": [
    "<h3>3.1 Open-ended Code:</h3><p>\n",
    "You may follow the steps in part 2 again but making innovative changes like creating new features, using new training algorithms, etc. Make sure you explain everything clearly in part 3.2. Note that reaching the 75% creative baseline is only a small portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DauIU5pgWNp2"
   },
   "outputs": [],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 3.2\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwAAxp-nWNp2"
   },
   "source": [
    "<h3>3.2 Explanation in Words:</h3><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8VSGimzWNp2"
   },
   "source": [
    "You need to answer the following questions in a markdown cell after this cell:\n",
    "\n",
    "3.2.1 How much did you manage to improve performance on the test set compared to part 2? Did you reach the 75% accuracy for the test in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "3.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLaBvS8xWNp2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEPzXDJKWNp2"
   },
   "source": [
    "<h2>Part 4: Kaggle Submission</h2><p>\n",
    "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The CSV shall contain TWO column named exactly \"FIPS\" and \"Result\" and 1555 total rows excluding the column names, \"FIPS\" column shall contain FIPS of counties with same order as in the test_2016_no_label.csv while \"Result\" column shall contain the 0 or 1 prdicaitons for corresponding columns. A sample predication file can be downloaded from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeaPBr5lWNp2"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# You may use pandas to generate a dataframe with FIPS and your predictions first \n",
    "# and then use to_csv to generate a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTXmf-A4WNp2"
   },
   "source": [
    "<h2>Part 5: Resources and Literature Used</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ33BTvJWNp2"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 4780 Final Project Student Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
