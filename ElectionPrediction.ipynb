{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVj1JpqUWNp1"
   },
   "source": [
    "<h2>CS 4780/5780 Final Project: </h2>\n",
    "<h3>Election Result Prediction for US Counties</h3>\n",
    "\n",
    "Names and NetIDs for your group members: Eric Osband (eo255), Anthony Cuturuffo (acc284), Eddie Freedman (ebf45???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4Y-2-S5WNp1"
   },
   "source": [
    "<h3>Introduction:</h3>\n",
    "\n",
    "<p> The final project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The programming project provide templates for how to do this, and the most recent video lectures summarize some of the tricks you will need (e.g. feature normalization, feature construction). So, this final project brings realism to how you will use machine learning in the real world.  </p>\n",
    "\n",
    "The task you will work on is forecasting election results. Economic and sociological factors have been widely used when making predictions on the voting results of US elections. Economic and sociological factors vary a lot among counties in the United States. In addition, as you may observe from the election map of recent elections, neighbor counties show similar patterns in terms of the voting results. In this project you will bring the power of machine learning to make predictions for the county-level election results using Economic and sociological factors and the geographic structure of US counties. </p>\n",
    "<p>\n",
    "\n",
    "<h3>Your Task:</h3>\n",
    "Plase read the project description PDF file carefully and make sure you write your code and answers to all the questions in this Jupyter Notebook. Your answers to the questions are a large portion of your grade for this final project. Please import the packages in this notebook and cite any references you used as mentioned in the project description. You need to print this entire Jupyter Notebook as a PDF file and submit to Gradescope and also submit the ipynb runnable version to Canvas for us to run.\n",
    "\n",
    "<h3>Due Date:</h3>\n",
    "The final project dataset and template jupyter notebook will be due on <strong>December 15th</strong> . Note that <strong>no late submissions will be accepted</strong>  and you cannot use any of your unused slip days before.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdQ0GV-eWNp1"
   },
   "source": [
    "<h2>Part 1: Basics</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFL54oJVWNp1"
   },
   "source": [
    "<h3>1.1 Import:</h3><p>\n",
    "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "    \n",
    "https://pytorch.org/tutorials/\n",
    "    \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM ERIC: To download PyTorch, run the following\n",
    "\n",
    "<code>conda install pytorch torchvision -c pytorch</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mQDt73qwWNp1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJGUskA_WNp1"
   },
   "source": [
    "<h3>1.2 Weighted Accuracy:</h3><p>\n",
    "Since our dataset labels are heavily biased, you need to use the following function to compute weighted accuracy throughout your training and validation process and we use this for testing on Kaggle.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vz7GtRvuWNp1"
   },
   "outputs": [],
   "source": [
    "def weighted_accuracy(pred, true):\n",
    "    assert(len(pred) == len(true))\n",
    "    num_labels = len(true)\n",
    "    num_pos = sum(true)\n",
    "    num_neg = num_labels - num_pos\n",
    "    frac_pos = num_pos/num_labels\n",
    "    weight_pos = 1/frac_pos\n",
    "    weight_neg = 1/(1-frac_pos)\n",
    "    num_pos_correct = 0\n",
    "    num_neg_correct = 0\n",
    "    for pred_i, true_i in zip(pred, true):\n",
    "        num_pos_correct += (pred_i == true_i and true_i == 1)\n",
    "        num_neg_correct += (pred_i == true_i and true_i == 0)\n",
    "    weighted_accuracy = ((weight_pos * num_pos_correct) \n",
    "                         + (weight_neg * num_neg_correct))/((weight_pos * num_pos) + (weight_neg * num_neg))\n",
    "    return weighted_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd8F4FVTWNp2"
   },
   "source": [
    "<h2>Part 2: Baseline Solution</h2><p>\n",
    "Note that your code should be commented well and in part 2.4 you can refer to your comments. (e.g. # Here is SVM, \n",
    "# Here is validation for SVM, etc). Also, we recommend that you do not to use 2012 dataset and the graph dataset to reach the baseline accuracy for 68% in this part, a basic solution with only 2016 dataset and reasonable model selection will be enough, it will be great if you explore thee graph and possibly 2012 dataset in Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4STDPiVFWNp2"
   },
   "source": [
    "<h3>2.1 Preprocessing and Feature Extraction:</h3><p>\n",
    "Given the training dataset and graph information, you need to correctly preprocess the dataset (e.g. feature normalization). For baseline solution in this part, you might not need to introduce extra features to reach the baseline test accuracy.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_target(df):\n",
    "    '''\n",
    "    add_target(df) is df but with a target column extracted from GOP and DEM vote counts.\n",
    "    Labels 1 if DEM > GOP and 0 otherwise. DEM and GOP columns are removed afterwards.\n",
    "    '''\n",
    "    df[\"target\"] = (df[\"DEM\"] > df[\"GOP\"]).astype(int)\n",
    "    df = df.drop(columns = [\"DEM\", \"GOP\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = add_target(pd.read_csv(\"./train_2016.csv\", sep=',', encoding='unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    add_features(df) is df but with the following additional features:\n",
    "        state: id corresponding to state (integer 0-49)\n",
    "    \"\"\"\n",
    "    # Get state initials from a county string, map state initials to their place in array, create new column\n",
    "    get_state_from_county = lambda county : county[county.index(\",\") + 2:]\n",
    "    df.loc[:,\"state_name\"] = df[\"County\"].apply(get_state_from_county)\n",
    "    states = df[\"state_name\"].unique().tolist()\n",
    "    get_id_from_state = lambda state : states.index(state)\n",
    "    df.loc[:,\"state\"] = df[\"state_name\"].apply(get_id_from_state)\n",
    "\n",
    "    # Get rid of all commas in MedianIncome column\n",
    "    df.loc[:,'MedianIncome']= df['MedianIncome'].str.replace(',','').astype(int)\n",
    "    \n",
    "    df = df.drop(columns = [\"County\",'state_name']) # These columns no longer needed\n",
    "    return df\n",
    "\n",
    "def preprocess(train_df, validation_df, test_df):\n",
    "    \"\"\"\n",
    "    preprocess(train_df, validation_df, test_df) returns the three respective dataframes but preprocessed\n",
    "    in the following way:\n",
    "        1) Add features as decided by add_features(df)\n",
    "        2) Normalize all features to a standard normal according to train_df statistics\n",
    "        3) Make target column the last column (only applies to train_df and validation_df)\n",
    "    \"\"\"\n",
    "    # First add features\n",
    "    train_df = add_features(train_df)\n",
    "    validation_df = add_features(validation_df)\n",
    "    test_df = add_features(test_df)\n",
    "\n",
    "    # Hold onto column references to put them back later\n",
    "    temp = train_df[\"target\"]\n",
    "    temp2 = validation_df[\"target\"]\n",
    "    train_df = train_df.drop(columns = [\"target\"])\n",
    "    validation_df = validation_df.drop(columns = [\"target\"])\n",
    "    \n",
    "    columns = list(train_df.columns)[1:]\n",
    "\n",
    "    std_scaler = StandardScaler()\n",
    "    std_scaler.fit(train_df[columns]) # Fit scaler to training dataset only\n",
    "    # Scale all three datasets\n",
    "    train_df[columns] = std_scaler.transform(train_df[columns]) \n",
    "    validation_df[columns] = std_scaler.transform(validation_df[columns])\n",
    "    test_df[columns] = std_scaler.transform(test_df[columns])\n",
    "    train_df[\"target\"] = temp # Add back target columns to ensure they are at the end of the dataframe\n",
    "    validation_df[\"target\"] = temp2\n",
    "    return train_df, validation_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>MedianIncome</th>\n",
       "      <th>MigraRate</th>\n",
       "      <th>BirthRate</th>\n",
       "      <th>DeathRate</th>\n",
       "      <th>BachelorRate</th>\n",
       "      <th>UnemploymentRate</th>\n",
       "      <th>state</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>13309</td>\n",
       "      <td>-1.603402</td>\n",
       "      <td>1.033455</td>\n",
       "      <td>-1.534324</td>\n",
       "      <td>-1.104511</td>\n",
       "      <td>-0.950331</td>\n",
       "      <td>2.038591</td>\n",
       "      <td>-1.444147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>6063</td>\n",
       "      <td>0.300453</td>\n",
       "      <td>1.304335</td>\n",
       "      <td>-1.493030</td>\n",
       "      <td>-0.124578</td>\n",
       "      <td>0.179436</td>\n",
       "      <td>2.468241</td>\n",
       "      <td>-1.360060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>29207</td>\n",
       "      <td>-0.958994</td>\n",
       "      <td>-0.424514</td>\n",
       "      <td>-0.254205</td>\n",
       "      <td>0.964237</td>\n",
       "      <td>-0.908096</td>\n",
       "      <td>0.695936</td>\n",
       "      <td>-1.275973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>51131</td>\n",
       "      <td>-0.828407</td>\n",
       "      <td>0.053509</td>\n",
       "      <td>-1.038794</td>\n",
       "      <td>1.798995</td>\n",
       "      <td>-0.021177</td>\n",
       "      <td>0.427405</td>\n",
       "      <td>-1.191886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>55043</td>\n",
       "      <td>-0.183841</td>\n",
       "      <td>-0.217371</td>\n",
       "      <td>-0.254205</td>\n",
       "      <td>-0.523810</td>\n",
       "      <td>0.126643</td>\n",
       "      <td>-0.646720</td>\n",
       "      <td>-1.107799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FIPS  MedianIncome  MigraRate  BirthRate  DeathRate  BachelorRate  \\\n",
       "1249  13309     -1.603402   1.033455  -1.534324  -1.104511     -0.950331   \n",
       "1306   6063      0.300453   1.304335  -1.493030  -0.124578      0.179436   \n",
       "768   29207     -0.958994  -0.424514  -0.254205   0.964237     -0.908096   \n",
       "981   51131     -0.828407   0.053509  -1.038794   1.798995     -0.021177   \n",
       "616   55043     -0.183841  -0.217371  -0.254205  -0.523810      0.126643   \n",
       "\n",
       "      UnemploymentRate     state  target  \n",
       "1249          2.038591 -1.444147       0  \n",
       "1306          2.468241 -1.360060       0  \n",
       "768           0.695936 -1.275973       0  \n",
       "981           0.427405 -1.191886       1  \n",
       "616          -0.646720 -1.107799       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test_2016_no_label.csv\")\n",
    "train_df, validation_df = train_test_split(df_2016, test_size=0.2) # Perform a test-train split of training data\n",
    "train_df = train_df.copy()\n",
    "validation_df = validation_df.copy()\n",
    "df, validation, test = preprocess(train_df, validation_df, test_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEGIN NEURAL NETWORK PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NN_dataset():\n",
    "    '''\n",
    "    Gets Neural Network dataset.\n",
    "    '''\n",
    "    df = pd.read_csv(\"./train_2016.csv\", sep=',', encoding='unicode_escape')\n",
    "    #looked at many online resources for neural networks and failed many times during implementation\n",
    "    #but converged to https://stackabuse.com/introduction-to-pytorch-for-classification/\n",
    "    #Used the model of the neural network and changed around the parameters, layers, weights, epoch\n",
    "    #size, dropout, and loss function. \n",
    "    #for better accuracy on our test set. \n",
    "    #preprocessing \n",
    "    return add_features(add_target(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['FIPS', 'MedianIncome', 'MigraRate', 'BirthRate', 'DeathRate',\n",
      "       'BachelorRate', 'UnemploymentRate', 'target', 'state'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset = get_NN_dataset()\n",
    "print(dataset.columns)\n",
    "#Label columns for whether they is a numerical value, or a non-numerical value for the case of the state name\n",
    "#which will be treated as an index \n",
    "categorical_columns = ['state']\n",
    "numerical_columns = ['MedianIncome', 'MigraRate', 'BirthRate', 'DeathRate', 'BachelorRate', 'UnemploymentRate']\n",
    "outputs = ['target']\n",
    "\n",
    "#convert to type category\n",
    "for category in categorical_columns:\n",
    "    dataset[category] = dataset[category].astype('category')\n",
    "statname = dataset['state'].cat.codes.values\n",
    "\n",
    "#creates respective tensors for categorical, numerical, and output data\n",
    "categorical_data = np.stack([statname], 1)\n",
    "categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
    "\n",
    "numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
    "numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
    "\n",
    "outputs = torch.tensor(dataset[outputs].values).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 25)]\n"
     ]
    }
   ],
   "source": [
    "#choosing embedding size by the number of unique states divided by 2\n",
    "categorical_column_sizes = [len(dataset[column].cat.categories) for column in categorical_columns]\n",
    "categorical_embedding_sizes = [(col_size, min(50, (col_size+1)//2)) for col_size in categorical_column_sizes]\n",
    "print(categorical_embedding_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set size: 1244, test set size: 311\n",
    "total_records = 1555\n",
    "test_records = int(total_records * .2)\n",
    "\n",
    "#partition training and validation set respectively\n",
    "categorical_train_data = categorical_data[:total_records-test_records]\n",
    "categorical_test_data = categorical_data[total_records-test_records:total_records]\n",
    "numerical_train_data = numerical_data[:total_records-test_records]\n",
    "numerical_test_data = numerical_data[total_records-test_records:total_records]\n",
    "train_outputs = outputs[:total_records-test_records]\n",
    "test_outputs = outputs[total_records-test_records:total_records]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG-1TlocWNp2"
   },
   "source": [
    "<h3>2.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
    "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** We apply the LDA in section 2.3 via validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_numerical_cols, output_size, layers, p=0.4):\n",
    "        super().__init__()\n",
    "        #sets up embedding with embedding size for state name\n",
    "        self.all_embeddings = nn.ModuleList([nn.Embedding(ni, nf) for ni, nf in embedding_size])\n",
    "        #dropout randomly zeros elements to avoid overfitting the training set\n",
    "        self.embedding_dropout = nn.Dropout(p)\n",
    "        #normalizes numerical data per batch \n",
    "        self.batch_norm_num = nn.BatchNorm1d(num_numerical_cols)\n",
    "\n",
    "        #calculates total input size for first layer of the nn\n",
    "        num_categorical_cols = sum((nf for ni, nf in embedding_size))\n",
    "        input_size = num_categorical_cols + num_numerical_cols\n",
    "\n",
    "        all_layers = []\n",
    "        #each layer has a ReLU activation along with Batch Normalization with dropout\n",
    "        for i in layers:\n",
    "            all_layers.append(nn.Linear(input_size, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(p))\n",
    "            input_size = i\n",
    "        \n",
    "        #finishes with last output layer\n",
    "        all_layers.append(nn.Linear(layers[-1], output_size))\n",
    "        #Creates the network\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "        \n",
    "    #forward pass\n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        #adds embedding for categorical columns\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.all_embeddings):\n",
    "            embeddings.append(e(x_categorical[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.embedding_dropout(x)\n",
    "        #applies batch normalization\n",
    "        x_numerical = self.batch_norm_num(x_numerical)\n",
    "        x = torch.cat([x, x_numerical], 1)\n",
    "        #performs the forward pass calculations\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 loss: 0.82673168\n",
      "epoch:  26 loss: 0.81152558\n",
      "epoch:  51 loss: 0.74759930\n",
      "epoch:  76 loss: 0.75068676\n",
      "epoch: 101 loss: 0.71078569\n",
      "epoch: 126 loss: 0.71594638\n",
      "epoch: 151 loss: 0.67671406\n",
      "epoch: 176 loss: 0.68952465\n",
      "epoch: 201 loss: 0.64570290\n",
      "epoch: 226 loss: 0.65024805\n",
      "epoch: 251 loss: 0.62783855\n",
      "epoch: 276 loss: 0.62232268\n",
      "epoch: 301 loss: 0.59600860\n",
      "epoch: 326 loss: 0.60050726\n",
      "epoch: 351 loss: 0.57287937\n",
      "epoch: 376 loss: 0.57107258\n",
      "epoch: 401 loss: 0.57092923\n",
      "epoch: 426 loss: 0.55099827\n",
      "epoch: 451 loss: 0.55500185\n",
      "epoch: 476 loss: 0.52533036\n",
      "epoch: 501 loss: 0.52303916\n",
      "epoch: 526 loss: 0.52062190\n",
      "epoch: 551 loss: 0.50915277\n",
      "epoch: 576 loss: 0.51518750\n",
      "epoch: 601 loss: 0.49213085\n",
      "epoch: 626 loss: 0.48122963\n",
      "epoch: 651 loss: 0.46421376\n",
      "epoch: 676 loss: 0.44645485\n",
      "epoch: 700 loss: 0.4527860582\n"
     ]
    }
   ],
   "source": [
    "#instantiates the model\n",
    "model = Model(categorical_embedding_sizes, numerical_data.shape[1], 2, [200,100,50,50], p=0.35)\n",
    "#sets CrossEntropyLoss to loss function with weights on the demovrats due to imbalanced data. \n",
    "#CrossEntropy Loss combines a Log Softmax layer with and negative log likelyhood loss in one single class\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.NLLLoss(weight = torch.Tensor([1.0, 1.1]))\n",
    "\n",
    "#uses Stochastic Gradient Descent, optionally included Nesterov momentum \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=5e-4, momentum=.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "#number of times to run through the training set\n",
    "epochs = 700\n",
    "\n",
    "#stores losses\n",
    "aggregated_losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    \n",
    "    #calculates loss off of model prediction\n",
    "    y_pred = model(categorical_train_data, numerical_train_data)\n",
    "    single_loss = loss_function(y_pred, train_outputs)\n",
    "    aggregated_losses.append(single_loss)\n",
    "\n",
    "    #prints loss\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "    \n",
    "    #backpropagation\n",
    "    optimizer.zero_grad() # First zero all the gradients because of the way pytorch works\n",
    "    single_loss.backward() # Perform backprop \n",
    "    optimizer.step() # performs a parameter update based on the current gradient\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.50656122\n",
      "Weighted accuracy: 0.68617374\n"
     ]
    }
   ],
   "source": [
    "#prints out testing loss\n",
    "with torch.no_grad():\n",
    "    y_val = model(categorical_test_data, numerical_test_data)\n",
    "    loss = loss_function(y_val, test_outputs)\n",
    "print(f'Loss: {loss:.8f}')\n",
    "#finds the max of the two ouputted nodes for the binary classification \n",
    "y_output = np.argmax(y_val.numpy(), axis=1)\n",
    "y_correct = test_outputs.numpy()\n",
    "print(\"Weighted accuracy:\",weighted_accuracy(y_output, test_outputs).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests the model on test data\n",
    "def get_NN_output(df):\n",
    "    df = add_features(df)\n",
    "    categorical_columns = ['state']\n",
    "    numerical_columns = ['MedianIncome', 'MigraRate', 'BirthRate', 'DeathRate', 'BachelorRate', 'UnemploymentRate']\n",
    "\n",
    "\n",
    "    for category in categorical_columns:\n",
    "        dataset[category] = dataset[category].astype('category')\n",
    "    statname = dataset['state'].cat.codes.values\n",
    "    categorical_data = np.stack([statname], 1)\n",
    "    categorical_data = torch.tensor(categorical_data, dtype=torch.int64)\n",
    "\n",
    "    numerical_data = np.stack([dataset[col].values for col in numerical_columns], 1)\n",
    "    numerical_data = torch.tensor(numerical_data, dtype=torch.float)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_val = model(categorical_data, numerical_data)\n",
    "\n",
    "    y_test_output = np.argmax(y_val, axis=1)\n",
    "    np_y_test_out = y_test_output.detach().numpy()\n",
    "    return np_y_test_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbJ4kRDhWNp2"
   },
   "source": [
    "<h3>2.3 Training, Validation and Model Selection:</h3><p>\n",
    "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "e3mQNv56WNp2"
   },
   "outputs": [],
   "source": [
    "# Now that we have added all necessary features, we perform validation to determine the optimal\n",
    "# prior probabilities.\n",
    "def perform_validation(df, validation):\n",
    "    '''\n",
    "    perform_validation(df, validation) performs validation on the prior probabilities of LDA to find the value\n",
    "    of the priors that maximize validation weighted accuracy.\n",
    "    Returns tuple:\n",
    "        best_lda, best_priors, best_score, best_one_percentages\n",
    "            best_lda: after finding optimal priors, this is an LDA fit on entire training + validation set \n",
    "            best_priors: array of length 2 with the optimal priors calculated through validation\n",
    "            best_score: weighted accuracy achieved by the highest performing lda on the validation set\n",
    "            best_one_percentages: using these optimal priors, this was the number of 1s predicted by the algorithm\n",
    "    '''\n",
    "    get_priors = lambda x : [0.9 - 0.001*x, 0.1 + 0.001*x] # Function to calculate priors from loop iteration\n",
    "    scores = []\n",
    "    one_percentages = []\n",
    "    x_train = df[df.columns[1:-1]] # Separate data into x and y train and test\n",
    "    x_test = validation[df.columns[1:-1]]\n",
    "    y_train = df['target']\n",
    "    y_test = validation['target']\n",
    "    # Now we repeatedly create LDA models after nudging the prior probabilities, then record the score\n",
    "    for x in range(899):\n",
    "        priors = get_priors(x)\n",
    "        lda = LinearDiscriminantAnalysis(priors = priors).fit(x_train,y_train)\n",
    "        y_pred = lda.predict(x_test)\n",
    "        accuracy = weighted_accuracy(y_pred, y_test)\n",
    "        scores.append(accuracy)\n",
    "        one_percentages.append(np.count_nonzero(y_test) / len(y_test))\n",
    "    # Now we see which priors and accuracy were most successful\n",
    "    x = np.argmax(scores)\n",
    "    priors = get_priors(x)\n",
    "    lda = LinearDiscriminantAnalysis(priors = priors).fit(pd.concat([x_train,x_test]),pd.concat([y_train,y_test]))\n",
    "    return lda, priors, scores[x],one_percentages[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best priors: [0.271, 0.729]\n",
      "Weighted accuracy: 0.7869623655913979\n",
      "Percentage of 1s predicted: 0.10289389067524116\n"
     ]
    }
   ],
   "source": [
    "lda_basic, priors, score, one_percentage = perform_validation(df, validation)\n",
    "basic_score = score # For use later to compare to creative\n",
    "print(\"Best priors:\",priors) # Priors are in the form [prior for GOP (0), prior for DEM (1)]\n",
    "print(\"Weighted accuracy:\", score) # Weighted accuracy score we achieved\n",
    "print(\"Percentage of 1s predicted:\", one_percentage) # We also look to see the percentage of 1s we predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuOP9JmuWNp2"
   },
   "source": [
    "<h3>2.4 Explanation in Words:</h3><p>\n",
    "    You need to answer the following questions in the markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHiJ7drqWNp2"
   },
   "source": [
    "2.4.1 How did you preprocess the dataset and features?\n",
    "\n",
    "2.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
    "\n",
    "2.4.3 How did you do the model selection?\n",
    "\n",
    "2.4.4 Does the test performance reach a given baseline 68% performanc? (Please include a screenshot of Kaggle Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pwkT1-tWNp2"
   },
   "source": [
    "1. To preprocess the label, we created a new binary value that equals 1 when the number of DEM votes was greater than the number of GOP votes, and 0 otherwise. In terms of features, we extracted the state abbreviation from the \"County\" column via substring selection and assigned each number to a number from 1 to 50. From there, all feature values (which excludes FIPS) were standardized according to a standard normal distribution.\n",
    "\n",
    "    For the neural network, I preprocessed the features by converting the name of the state in which the county was in into an index to add as a feature. I did this using the Embedding Libraries that pytorch has supplied for me. I also converted the income feature into an integer since it was primarily a string. I also made the target equal to 1 for Democrat and 0 for GOP to complete the binary classification. This calculation was made by finding the majority of Democrats or GOP. \n",
    "\n",
    "\n",
    "2. We chose to use LDA and Neural Networks as our two learning methods. \n",
    "    We chose LDA because we believed many of the features to be independently generated. It could also take into account the prior class probabilities, which in the end allowed us to essentially assign a greater weight to predict 1 than 0, and thus gave us such good accuracy. Additionally, most of our data was real-valued, so it was quite practical. These characteristics allowed us to assume that our data was normally distributed, which is an assumption of the LDA method. And clearly the model was quite a success.\n",
    "    \n",
    "    We choose neural network because they are “universal aproximators”. If implemented correctly, they can approximate any dataset. I was previously aware of the fact there were plenty of good machine learning frameworks that were built to make the process of implementing a neural network easier. This definitely helped the decision process. Another good feature of neural networks is that if the results were not as good as hoped, they allow us to change around the parameters to better fit the dataset. \n",
    "\n",
    "\n",
    "3. For the LDA model selection, we tested 899 different LDA's with slightly different priors and tested their accuracy on our validation set (20% of the total sample). Afterwards, we selected the model/parameters that achieved the greatest validation score. We also played around with different solvers, as well as using quadratic analysis rather than linear, but none of those actually increased our performance by much so we kept it as it is now. \n",
    "\n",
    "    For the neural network, I partitioned the training set into a training set and a validation set. When tuning parameters, I would train on the training set and validated the effect of changing certain parameters (learning rate, size of hidden layers, number of layers, loss function) on the validation set. I began with NLLLoss but changed to CrossEntropyLoss to add the softmax hidden layer. Additionally, I started using Stochastic Gradient Descent, but found that it was not learning fast enough and after I switched to torch's Adam optimizer, it was learning much faster per epoch.\n",
    "    \n",
    "\n",
    "4. Our test performance far exceeded baseline of 68% (for both NN and LDA):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BasicScore.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2JwrNuEWNp2"
   },
   "source": [
    "<h2>Part 3: Creative Solution</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HttJ6XeDWNp2"
   },
   "source": [
    "<h3>3.1 Open-ended Code:</h3><p>\n",
    "You may follow the steps in part 2 again but making innovative changes like creating new features, using new training algorithms, etc. Make sure you explain everything clearly in part 3.2. Note that reaching the 75% creative baseline is only a small portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary datasets and add target column when appropriate\n",
    "df_2016 = add_target(pd.read_csv(\"./train_2016.csv\", sep=',', encoding='unicode_escape'))\n",
    "df_graph = pd.read_csv(\"./graph.csv\", sep=',', encoding='unicode_escape')\n",
    "df_2012 = add_target(pd.read_csv(\"./train_2012.csv\", sep=',', encoding='unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "O7bsqEl8WNp2",
    "outputId": "8fa672e5-45b1-4982-f259-1ffe5710947d"
   },
   "outputs": [],
   "source": [
    "def add_features_creative(df):\n",
    "    \"\"\"\n",
    "    add_features_creative(df) is df but with the following additional features:\n",
    "        avg_neighbor: average prediction of all neighboring counties to the given county.\n",
    "                      Neighbors determined by graph.csv. First looks at prediction from 2016\n",
    "                      election, and if that cannot be found, looks at data from 2012 election.\n",
    "                      If that cannot be found either, then the given neighboring county is \n",
    "                      discounted from the average county score.\n",
    "         num_neighbors: number of contiguous counties to the given one, as given by graph.csv\n",
    "    \"\"\"    \n",
    "    avg_neighbor = []\n",
    "    num_neighbors = []\n",
    "    for row in df.iterrows():\n",
    "        # For each row, find all of its neighbors' FIPS codes as given in graph.csv\n",
    "        fips = row[1][\"FIPS\"]\n",
    "        neighbors1 = df_graph.loc[df_graph[\"SRC\"] == fips,\"DST\"].to_numpy()\n",
    "        neighbors2 = df_graph.loc[df_graph[\"DST\"] == fips,\"SRC\"].to_numpy()\n",
    "        neighbors = np.append(neighbors1,neighbors2)\n",
    "        neighbors = np.delete(neighbors,np.where(neighbors == fips)) # Delete current county from neighbors list\n",
    "        num_neighbors.append(len(neighbors)) # Add number of neighbors\n",
    "        total = 0\n",
    "        count = 0\n",
    "        for neighbor in neighbors:\n",
    "            count += 1\n",
    "            ndf = df_2016[df_2016[\"FIPS\"] == neighbor].head(1)\n",
    "            if not ndf.empty: # If this county was in 2016 dataset, add its target value to count\n",
    "                total += ndf.iloc[0][\"target\"]\n",
    "            else: # If it was not, then look at 2012 data\n",
    "                ndf = df_2012[df_2012[\"FIPS\"] == neighbor].head(1)\n",
    "#                 if not ndf.empty: # Add 2012 data if found\n",
    "                if False:\n",
    "                    total += ndf.iloc[0][\"target\"]\n",
    "                else: # If neither was found, make sure count keeps up\n",
    "                    count -= 1\n",
    "        if count <= 0:             # If there was no county information, put in a placeholder value of 0 as average\n",
    "            avg_neighbor.append(0) # neighbor score. This makes sense because most likely the missing county will\n",
    "        else:                      # be rural and therefore its neighbors would have voted GOP.\n",
    "            avg_neighbor.append(total / count)  # Average only reported counties\n",
    "\n",
    "    df.loc[:,\"avg_neighbor\"] = avg_neighbor\n",
    "    df.loc[:,\"num_neighbors\"] = num_neighbors\n",
    "    return df\n",
    "\n",
    "def preprocess_creative(train_df, validation_df, test_df):\n",
    "    \"\"\"\n",
    "    preprocess_creative(train_df, validation_df, test_df) returns the three respective dataframes but preprocessed\n",
    "    in the following way:\n",
    "        1) Add features as decided by add_features_creative(df)\n",
    "        2) Normalize all features to a standard normal according to train_df statistics\n",
    "        3) Make target column the last column (only applies to train_df and validation_df)\n",
    "    \"\"\"\n",
    "    # First add creative features\n",
    "    train_df = add_features_creative(train_df)\n",
    "    validation_df = add_features_creative(validation_df)\n",
    "    test_df = add_features_creative(test_df)\n",
    "    # After adding creative features, we go and do exactly what we did in the basic solution to normalize, etc.\n",
    "    return preprocess(train_df, validation_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>MedianIncome</th>\n",
       "      <th>MigraRate</th>\n",
       "      <th>BirthRate</th>\n",
       "      <th>DeathRate</th>\n",
       "      <th>BachelorRate</th>\n",
       "      <th>UnemploymentRate</th>\n",
       "      <th>avg_neighbor</th>\n",
       "      <th>num_neighbors</th>\n",
       "      <th>state</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>45045</td>\n",
       "      <td>0.466271</td>\n",
       "      <td>0.979770</td>\n",
       "      <td>0.526584</td>\n",
       "      <td>-0.782416</td>\n",
       "      <td>1.325086</td>\n",
       "      <td>-0.443486</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>1.533789</td>\n",
       "      <td>-1.566346</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>27133</td>\n",
       "      <td>0.631568</td>\n",
       "      <td>-0.395240</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.508143</td>\n",
       "      <td>0.137221</td>\n",
       "      <td>-1.581476</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>-1.480097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>31181</td>\n",
       "      <td>-0.640721</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>-0.971415</td>\n",
       "      <td>1.659182</td>\n",
       "      <td>0.094797</td>\n",
       "      <td>-1.126280</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.776332</td>\n",
       "      <td>-1.393848</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>51035</td>\n",
       "      <td>-0.738811</td>\n",
       "      <td>0.036450</td>\n",
       "      <td>-1.335793</td>\n",
       "      <td>0.403503</td>\n",
       "      <td>-0.912767</td>\n",
       "      <td>0.068609</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.776332</td>\n",
       "      <td>-1.307599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>8051</td>\n",
       "      <td>0.330337</td>\n",
       "      <td>1.163638</td>\n",
       "      <td>-0.971415</td>\n",
       "      <td>-2.107855</td>\n",
       "      <td>3.562938</td>\n",
       "      <td>-1.581476</td>\n",
       "      <td>0.770794</td>\n",
       "      <td>1.533789</td>\n",
       "      <td>-1.221350</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FIPS  MedianIncome  MigraRate  BirthRate  DeathRate  BachelorRate  \\\n",
       "905   45045      0.466271   0.979770   0.526584  -0.782416      1.325086   \n",
       "1135  27133      0.631568  -0.395240   0.000260   0.508143      0.137221   \n",
       "346   31181     -0.640721   0.012467  -0.971415   1.659182      0.094797   \n",
       "348   51035     -0.738811   0.036450  -1.335793   0.403503     -0.912767   \n",
       "1335   8051      0.330337   1.163638  -0.971415  -2.107855      3.562938   \n",
       "\n",
       "      UnemploymentRate  avg_neighbor  num_neighbors     state  target  \n",
       "905          -0.443486     -0.484891       1.533789 -1.566346       0  \n",
       "1135         -1.581476     -0.484891       0.018876 -1.480097       0  \n",
       "346          -1.126280     -0.484891       0.776332 -1.393848       0  \n",
       "348           0.068609     -0.484891       0.776332 -1.307599       0  \n",
       "1335         -1.581476      0.770794       1.533789 -1.221350       1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test_2016_no_label.csv\")\n",
    "train_df, validation_df = train_test_split(df_2016, test_size=0.2) # Perform a test-train split of training data\n",
    "train_df = train_df.copy()\n",
    "validation_df = validation_df.copy()\n",
    "df, validation, test_creative = preprocess_creative(train_df, validation_df, test_df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "DE0InbZ7WNp2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best priors: [0.387, 0.613]\n",
      "Weighted accuracy: 0.8016711833785005\n",
      "Percentage of 1s predicted: 0.13183279742765272\n"
     ]
    }
   ],
   "source": [
    "lda_creative, priors, score, one_percentage = perform_validation(df, validation)\n",
    "print(\"Best priors:\",priors) # Priors are in the form [prior for GOP (0), prior for DEM (1)]\n",
    "print(\"Weighted accuracy:\", score) # Weighted accuracy score we achieved\n",
    "print(\"Percentage of 1s predicted:\", one_percentage) # We also look to see the percentage of 1s we predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative algorithm performed 1.471% better than basic solution\n"
     ]
    }
   ],
   "source": [
    "# Note that the following number changes drastically from run to run\n",
    "print(f'Creative algorithm performed {(score - basic_score)*100:.3f}% better than basic solution',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwAAxp-nWNp2"
   },
   "source": [
    "<h3>3.2 Explanation in Words:</h3><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8VSGimzWNp2"
   },
   "source": [
    "You need to answer the following questions in a markdown cell after this cell:\n",
    "\n",
    "3.2.1 How much did you manage to improve performance on the test set compared to part 2? Did you reach the 75% accuracy for the test in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "3.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLaBvS8xWNp2"
   },
   "source": [
    "1) We managed to improve performance a significant amount. What was most surprising was that we always thought the neural network would by far surpass the accuracy of any other type of model, and so we initially created our basic LDA just as a dummy essentially. But after its success on the basic solution, we decided to delve deeper and see how far we could push our accuracy. In the end, after solving many, many bugs, we ended up reaching 84% accuracy on Kaggle, as seen below, compared to around 77% on the basic solution at the time of this writing.\n",
    "<img src=\"creative_accuracy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) To do this, we tried a couple of things, but ended up including all but one of them in our basic solution as described above. Thus the only unique element we added was incorporating both graph and 2012 data into our algorithm. We first hypothesized that the way surrounding counties voted would probably affect the way the county itself voted. Additionally, we also realized that the *number* of neighboring counties might also give us useful information, since many GOP counties tend to be square-shaped (thus having only 4 neighbors) whereas DEM counties might have weird squiggles and curves and thus would have more neighbors. To implement this, we added two new features: `avg_neighbor` and `num_neighbors`.\n",
    "\n",
    "`num_neighbors` is simply the number of neighbors listed in the graph.csv file. If you look at our code, we had to do an awkward array concatenation since the CSV file could have the given county in either column. Additionally, we had to make sure to remove the county itself from that list, as it was (for some reason) listed as one of its own neighbors. This interestingly gave us a significant bug wherein we were getting around 97% accuracy on our validation but only 73% on Kaggle, but eventually this bug was spotted and the county itself was discarded from computation.\n",
    "\n",
    "`avg_neighbor` is the average prediction of the given county's neighboring counties in either 2016 or 2012. The process was also described above in the code, but we'll explain it here again for ease of the reader. After we had assembled the list of all neighboring county codes, we looked through this list one-by-one and saw whether or not that county was listed in our entire train_2016 dataset. If it was, we added the target 1-0 value to our sum. If it was not in our table, then we looked at the train_2012 dataset, and if it was there, we added that target value to our sum. If it was not there, we did nothing. After we had collected this sum, we divided it by the number of counties found (in either 2016 or 2012 data) to obtain our average neighbor target value. So if all of a given county's neighbors voted democratic in 2016, then its average neighbor score would be 1.\n",
    "Note that this does not actually cause data-leakage into our validation set as we never use the target information of a training example itself, only of its neighbors.\n",
    "\n",
    "To summarize: we had hypotheses about two things that would probably be good features for our dataset, so we added them. We only resorted to using the 2012 dataset when we absolutely had to, i.e. the given county was not available in our 2016 sample. Other than that, we figured adding the 2012 dataset as a whole might bias our predictions a great deal, since we know from experience that voting lines changed a great deal between 2012 and 2016.\n",
    "\n",
    "After we assembled lists of each of these features, we added them to their respective dataframes, and normalized the data with a standard scaler as before. Surprisingly enough, adding just these two features managed to increase our prediction score by 7% or so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEPzXDJKWNp2"
   },
   "source": [
    "<h2>Part 4: Kaggle Submission</h2><p>\n",
    "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The CSV shall contain TWO column named exactly \"FIPS\" and \"Result\" and 1555 total rows excluding the column names, \"FIPS\" column shall contain FIPS of counties with same order as in the test_2016_no_label.csv while \"Result\" column shall contain the 0 or 1 prdicaitons for corresponding columns. A sample predication file can be downloaded from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>MedianIncome</th>\n",
       "      <th>MigraRate</th>\n",
       "      <th>BirthRate</th>\n",
       "      <th>DeathRate</th>\n",
       "      <th>BachelorRate</th>\n",
       "      <th>UnemploymentRate</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17059</td>\n",
       "      <td>-0.805827</td>\n",
       "      <td>-0.814899</td>\n",
       "      <td>-0.336793</td>\n",
       "      <td>0.637593</td>\n",
       "      <td>-1.256529</td>\n",
       "      <td>1.340411</td>\n",
       "      <td>-1.444147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6103</td>\n",
       "      <td>-0.730743</td>\n",
       "      <td>0.173014</td>\n",
       "      <td>0.489090</td>\n",
       "      <td>-0.124578</td>\n",
       "      <td>-0.665249</td>\n",
       "      <td>1.071879</td>\n",
       "      <td>-1.360060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42047</td>\n",
       "      <td>-0.044727</td>\n",
       "      <td>-0.711328</td>\n",
       "      <td>-0.791029</td>\n",
       "      <td>0.819062</td>\n",
       "      <td>-0.337934</td>\n",
       "      <td>0.212580</td>\n",
       "      <td>-1.275973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47147</td>\n",
       "      <td>0.682660</td>\n",
       "      <td>0.619169</td>\n",
       "      <td>0.447796</td>\n",
       "      <td>-0.378635</td>\n",
       "      <td>-0.337934</td>\n",
       "      <td>-0.539307</td>\n",
       "      <td>-1.191886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39039</td>\n",
       "      <td>0.187077</td>\n",
       "      <td>-0.392646</td>\n",
       "      <td>-0.212911</td>\n",
       "      <td>-0.197165</td>\n",
       "      <td>-0.538547</td>\n",
       "      <td>-0.109657</td>\n",
       "      <td>-1.107799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FIPS  MedianIncome  MigraRate  BirthRate  DeathRate  BachelorRate  \\\n",
       "0  17059     -0.805827  -0.814899  -0.336793   0.637593     -1.256529   \n",
       "1   6103     -0.730743   0.173014   0.489090  -0.124578     -0.665249   \n",
       "2  42047     -0.044727  -0.711328  -0.791029   0.819062     -0.337934   \n",
       "3  47147      0.682660   0.619169   0.447796  -0.378635     -0.337934   \n",
       "4  39039      0.187077  -0.392646  -0.212911  -0.197165     -0.538547   \n",
       "\n",
       "   UnemploymentRate     state  \n",
       "0          1.340411 -1.444147  \n",
       "1          1.071879 -1.360060  \n",
       "2          0.212580 -1.275973  \n",
       "3         -0.539307 -1.191886  \n",
       "4         -0.109657 -1.107799  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First examine previously trained test set to ensure all correct features are there and in the proper order\n",
    "test.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>MedianIncome</th>\n",
       "      <th>MigraRate</th>\n",
       "      <th>BirthRate</th>\n",
       "      <th>DeathRate</th>\n",
       "      <th>BachelorRate</th>\n",
       "      <th>UnemploymentRate</th>\n",
       "      <th>avg_neighbor</th>\n",
       "      <th>num_neighbors</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17059</td>\n",
       "      <td>-0.799297</td>\n",
       "      <td>-0.834923</td>\n",
       "      <td>-0.323631</td>\n",
       "      <td>0.577903</td>\n",
       "      <td>-1.252157</td>\n",
       "      <td>1.434196</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>-1.566346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6103</td>\n",
       "      <td>-0.723210</td>\n",
       "      <td>0.156363</td>\n",
       "      <td>0.486098</td>\n",
       "      <td>-0.154577</td>\n",
       "      <td>-0.658225</td>\n",
       "      <td>1.149698</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>-1.480097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42047</td>\n",
       "      <td>-0.028019</td>\n",
       "      <td>-0.730998</td>\n",
       "      <td>-0.768983</td>\n",
       "      <td>0.752303</td>\n",
       "      <td>-0.329441</td>\n",
       "      <td>0.239307</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>-1.393848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47147</td>\n",
       "      <td>0.709096</td>\n",
       "      <td>0.604041</td>\n",
       "      <td>0.445612</td>\n",
       "      <td>-0.398736</td>\n",
       "      <td>-0.329441</td>\n",
       "      <td>-0.557285</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.776332</td>\n",
       "      <td>-1.307599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39039</td>\n",
       "      <td>0.206885</td>\n",
       "      <td>-0.411228</td>\n",
       "      <td>-0.202172</td>\n",
       "      <td>-0.224337</td>\n",
       "      <td>-0.530953</td>\n",
       "      <td>-0.102090</td>\n",
       "      <td>-0.484891</td>\n",
       "      <td>0.018876</td>\n",
       "      <td>-1.221350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    FIPS  MedianIncome  MigraRate  BirthRate  DeathRate  BachelorRate  \\\n",
       "0  17059     -0.799297  -0.834923  -0.323631   0.577903     -1.252157   \n",
       "1   6103     -0.723210   0.156363   0.486098  -0.154577     -0.658225   \n",
       "2  42047     -0.028019  -0.730998  -0.768983   0.752303     -0.329441   \n",
       "3  47147      0.709096   0.604041   0.445612  -0.398736     -0.329441   \n",
       "4  39039      0.206885  -0.411228  -0.202172  -0.224337     -0.530953   \n",
       "\n",
       "   UnemploymentRate  avg_neighbor  num_neighbors     state  \n",
       "0          1.434196     -0.484891       0.018876 -1.566346  \n",
       "1          1.149698     -0.484891       0.018876 -1.480097  \n",
       "2          0.239307     -0.484891       0.018876 -1.393848  \n",
       "3         -0.557285     -0.484891       0.776332 -1.307599  \n",
       "4         -0.102090     -0.484891       0.018876 -1.221350  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_creative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(fname, preds):\n",
    "    outputdf = pd.DataFrame(test[\"FIPS\"]) # Create dataframe with one column of FIPS county codes\n",
    "    outputdf['Result'] = preds # Use our previously trained LDA to predict election result\n",
    "    # Then view the percentage of 1s predicted to ensure its not astronomically different from the number of \n",
    "    # 1s predicted on our training or validation sets, and thus reinforce our hypothesis that LDA is an accurate\n",
    "    # tool for modeling this data.\n",
    "    print(\"Percentage of 1s predicted:\",np.count_nonzero(outputdf['Result']) / len(outputdf['Result']))\n",
    "    outputdf.to_csv(fname, index = False) # Save information in this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(creative LDA)\n",
      "Percentage of 1s predicted: 0.28038585209003214\n",
      "(basic LDA)\n",
      "Percentage of 1s predicted: 0.4090032154340836\n",
      "(basic NN)\n",
      "Percentage of 1s predicted: 0.17363344051446947\n"
     ]
    }
   ],
   "source": [
    "test_features = test[test.columns[1:]] # Extract features from test set\n",
    "test_features_creative = test_creative[test_creative.columns[1:]] # Extract features from test set\n",
    "print(\"(creative LDA)\")\n",
    "save_to_csv(\"CreativeLDA\",lda_creative.predict(test_features_creative))\n",
    "print(\"(basic LDA)\")\n",
    "save_to_csv(\"BasicLDA\",lda_basic.predict(test_features))\n",
    "print(\"(basic NN)\")\n",
    "save_to_csv(\"BasicNN\",get_NN_output(pd.read_csv(\"test_2016_no_label.csv\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTXmf-A4WNp2"
   },
   "source": [
    "<h2>Part 5: Resources and Literature Used</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ33BTvJWNp2"
   },
   "source": [
    "We only used stackoverflow and documentation. **POTENTIALLLY NN EXAMPLE @ANTHONY??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, see preprocessing for neural network for credits for that."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 4780 Final Project Student Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
